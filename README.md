# Simple real time analytics demo (WIP)

## Objective 
To build a simple example of getting real time analytics (mean and standard deviation) of a simulated IoT sensor values (temperature/soil nitrogen content/moisture content etc). 

## Constraint
All the values generated since the sensor began operation are not stored in a database (we can choose to store them too, millions of entries too can be easily manageable by modern databases). Only the final analytics outputs are stored. Also the computation mechanism must accommodate for moving mean and standard deviation which should be the same as the true mean and std dev if we stored all the sensor values but there is no big machine available with enough RAM to accommodate millions of values generated by the sensor over time, hence the calculations must support an incremental approach. 

## Assumptions
The sensor operates 24/7 and it will work forever.

## Sample output
Coming soon...

## What I aim to learn out of this pet project
Data Engineering using queue based solutions and an intuitive solution for incremental and real time analytics. Some math about the incremental mean and std dev. Also websockets with a fanout pattern. 

## Methodology
We simulate an IoT sensor producing some value at fixed time intervals and our aim is to build a system which is real time in nature and provides us with the mean and standard deviation of the values generated till date. As per the constraints, we can't store all values generated so we must simulate streaming analytics with incremental updates. We use special formulas for mean and std dev calculations which support incoming new data (1 data point) and their output equals to the actual mean and std dev if all values are stored. The generation process is a simple random number generation written in python which acts as a Producer for the queue. The producer throws all the values on the queue (we use kafka as the event bus here). A consumer on the other end subscribes to this queue, listens to incoming messages, consumes them, does the calculation and stores them in a database (redis). These values are also then published to another queue which is meant for the updates to be sent to a simple websocket based Javascript backend server. A simple frontend displays the mean and standard deviation in an updating fashion with the websocket connection.   

## Tech stack used
* Python for writing the producer and consumer codebase. 
* Javacript (node & express) for the websocket(socketio) backend.
* React for the frontend.
* Kafka as an event bus (can easily use RabbitMQ, NATS, NATS streaming, redis pub/sub etc. instead)
* Redis as a database
* Docker to containerize all individual deplyoments
* Kubernetes (k3s) for orchestration 

## Steps to follow along and replicate this project

All these modules can be run in a single machine. I run all this in a Google Cloud VM with 2 vCPU and 7.5 GB RAM with Ubuntu 18.04. 

### Installing k3s

### Installing and configuring helm

### Installing docker

### Running a local docker container registry

### Deploying kafka in the k3s cluster

### Deploying redis in the k3s cluster

### Running the consumer in the k3s cluster

### Running the producer in the k3s cluster

### Testing the event flow between the producer and consumer flow

### Deploying the socketio backend in the k3s cluster and exposing it to the world


### Deploying the frontend in the k3s cluster and exposing it to the world


## Output


## TODO


## Word of caution
This is by no means a production level codebase and system design. If running locally, this system might slow down your computer even with 16 GB RAM, so be careful while deploying it.


## Helpful links
* [Socketio integration with kafkajs](https://stackoverflow.com/questions/66337792/how-do-i-connect-kafkajs-with-socket-io)